{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'generation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-314ef537a068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiviner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_functions1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresponser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'generation'"
     ]
    }
   ],
   "source": [
    "from generation.generation import diviner\n",
    "from my_functions1 import extractor\n",
    "from my_functions import responser\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/gpt-2-Pytorch\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/Student\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/pytorch_transformers\")\n",
    "\n",
    "from cam_summarize import load_cam_model\n",
    "from text_gen_big import load_big_model\n",
    "from text_gen import load_small_model\n",
    "\n",
    "#device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#LM_CAM = load_cam_model(device)\n",
    "#Cam = diviner(tp = 'cam', model = LM_CAM, device = device)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "LM_SMALL = load_small_model(device)\n",
    "GPT2Small = diviner(tp = 'small', model = LM_SMALL, device = device)\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "LM_BIG, tokenizer_big = load_big_model(device)\n",
    "GPT2Big = diviner(tp = 'big', model = LM_BIG, tokenizer = tokenizer_big, device = device)\n",
    "\n",
    "Templ = diviner(tp = 'templates', model = '', device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_template_answers = []\n",
    "list_of_given_qwestions = [\"What is better Toyota or Honda ?\", \"What is better Nokia or Motorolla ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "in extractor get params 0\n",
      "LayerContextWordEmbeddingsBert 2\n",
      "LayerContextWordEmbeddingsBert2 2\n",
      "LayerContextWordEmbeddingsBert3 2\n",
      "LayerContextWordEmbeddingsBert4 2\n",
      "LayerContextWordEmbeddingsBert5 2\n",
      "LayerContextWordEmbeddingsBert6 2\n",
      "LayerContextWordEmbeddingsBert7 2\n",
      "LayerContextWordEmbeddingsBert8 2\n",
      "LayerContextWordEmbeddingsBert9 2\n",
      "extract_objects_predicates tags [['O', 'O', 'B-PREDFULL', 'O', 'O', 'O', 'B-OBJ', 'O', 'O']]\n",
      "['python']\n",
      "['better']\n",
      "We try to use spacy\n",
      "split_sent ['what', 'is', 'better', 'for', 'deep', 'learning', 'python', 'or', 'matlab?']\n",
      "tokens  ['What', 'is', 'better', 'for', 'deep', 'learning', 'Python', 'or', 'Matlab', '?']\n",
      "or simple split_sent 7\n",
      "Python Matlab\n",
      "9\n",
      "len(obj1), len(obj2) 6 6\n",
      "obj1, obj2, predicates Python Matlab ['better']\n",
      "aspects ['better']\n",
      "weights [1]\n",
      "get url\n",
      "params {'objectA': 'Python', 'objectB': 'Matlab', 'fs': 'true', 'aspect1': 'better', 'weight1': 1}\n",
      "create fro json predicates ['better']\n",
      "create fro json self.predicates better\n",
      "aspects  ['quicker to develop code', 'quicker', 'matplotlib', 'easier'] ['faster', 'better for scientific computing', 'experience']\n",
      "2\n",
      "winnder: python  other: matlab\n",
      "acpect winner  quicker to develop code, quicker, matplotlib, easier\n",
      "acpect other  faster, better for scientific computing, experience\n",
      "type  templates\n",
      "winnder: python  other: matlab\n",
      "acpect winner  quicker to develop code, quicker, matplotlib, easier\n",
      "acpect other  faster, better for scientific computing, experience\n",
      "self predicate  better\n",
      "answer begin:  The python is better than matlab. The reason is quicker to develop code, quicker, matplotlib, easier. \n",
      "self type templates\n",
      "gen templates\n",
      "gen templates 2\n",
      "generate template 0\n",
      "generate template 1\n",
      "generate template 2\n",
      "gen templates 2 i came to the conclusion that python is better, because: quicker to develop code, quicker, matplotlib, easier.. But i should tell you that matlab is: faster, better for scientific computing, experience.\n",
      "full answer  i came to the conclusion that python is better, because: quicker to develop code, quicker, matplotlib, easier.. But i should tell you that matlab is: faster, better for scientific computing, experience.The reason is quicker to develop code, quicker, matplotlib, easier. \n",
      "answer0 i came to the conclusion that python is better, because: quicker to develop code, quicker, matplotlib, easier.. But i should tell you that matlab is: faster, better for scientific computing, experience.\n"
     ]
    }
   ],
   "source": [
    "my_diviner = Templ\n",
    "\n",
    "\n",
    "answer = generate_answer(my_extractor, my_diviner, \"What is better for deep learning Python or Matlab?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "  \n",
    "# reading csv file  \n",
    "ds = pd.read_csv(\"Evaluation dataset - Sheet1 (1).csv\") \n",
    "list_of_target1 = ds['Q1']\n",
    "list_of_target1 = ds['Q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would prefer a Laptop. You can:\\nIf you are buying a laptop with a fine Graphics Card, play games that a smartphone cant support.\\nGo coding. Laptops provide a much broader variety of scope as you can make it a Virtual Box(having two or more Operating systems) and download open source pgms like anaconda, open cv,etc.\\nBigger Screen. Watching movies takes a huge step forward when its a bigger screen and better sound effects. You can argue that the latest smartphones have all these capabilities but the average power drain comparison puts laptops ahead.\\nHuge storage space. Your 500GB or higher internal hard drive can store all the required stuff you will need.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_target1[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out\n",
      "exist3\n",
      "exist4\n",
      "exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 22.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qu\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  If you need to make a quick change in your code and you need to quickly run the test suite, Matlab is better than Python because it is faster to develop and run.\n",
      "\n",
      "There are many other factors that I haven't mentioned here, but I hope that you will find this list of Python vs. Matlab differences interesting.\n",
      "\n",
      "If you want to learn more about Python, I would highly recommend you to check out my book, Python for Beginners. It's free and I've written it for you. But if you are looking for something more practical, then I would recommend taking a look at the Python for Business book.\n",
      "\n",
      "If you have any questions, please don't hesitate to ask them in the comment section below.\n",
      "\n",
      "Thanks for reading!<|endoftext|>The last time I was in the US, I found myself in a small bar in a tiny town in the middle of nowhere, talking with an American who was talking about his first trip to the US. He\n",
      "qu2\n",
      "qu3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" If you need to make a quick change in your code and you need to quickly run the test suite, Matlab is better than Python because it is faster to develop and run.\\n\\nThere are many other factors that I haven't mentioned here, but I hope that you will find this list of Python vs. Matlab differences interesting.\\n\\nIf you want to learn more about Python, I would highly recommend you to check out my book, Python for Beginners. It's free and I've written it for you. But if you are looking for something more practical, then I would recommend taking a look at the Python for Business book.\\n\\nIf you have any questions, please don't hesitate to ask them in the comment section below.\\n\\nThanks for reading!<|endoftext|>The last time I was in the US, I found myself in a small bar in a tiny town in the middle of nowhere, talking with an American who was talking about his first trip to the US. He\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "text_generator_for_out(\"Python is better than Matlab, because it is quicker to develop code, easier.\", LM_BIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "\n",
    "def text_generator_for_out(text, model, length = 200, temperature = 0.7, top_k = 40):\n",
    "    print(\"text_generator_for_out\")\n",
    "    \n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print (\"exist3\")\n",
    "    # Load Model\n",
    "    enc = get_encoder()\n",
    "    print (\"exist4\")\n",
    "\n",
    "    quiet = False\n",
    "    print (\"exist\")\n",
    "    length = 200\n",
    "\n",
    "    if length == -1:\n",
    "        length = 1024 // 2\n",
    "    elif length > 1024:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % 1024)\n",
    "\n",
    "    context_tokens = enc.encode(text)\n",
    "\n",
    "    generated = 0\n",
    "    for _ in range(1):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=length,\n",
    "            context=context_tokens,\n",
    "            start_token=None,\n",
    "            batch_size=1,\n",
    "            temperature=temperature, top_k=top_k, device=device\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(1):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            if quiet is False:\n",
    "                print (\"qu\")\n",
    "                print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(\"in big gen2\", text)\n",
    "            print (\"qu2\")\n",
    "            print (\"qu3\")\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i came to the conclusion that python is better, because: quicker to develop code, quicker, matplotlib, easier.. But i should tell you that matlab is: faster, better for scientific computing, experience.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "URL = 'http://ltdemos.informatik.uni-hamburg.de/cam-api'\n",
    "#proxies = {\"http\": \"http://185.46.212.97:10015/\",\"https\": \"https://185.46.212.98:10015/\",}\n",
    "params = {\n",
    "            'objectA': 'Moscow',\n",
    "            'objectB': 'London',\n",
    "            'fs': str(True).lower()}\n",
    "response = requests.get(url=URL, params=params)#, proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "list_qw = []\n",
    "   \n",
    "with open('/notebook/dataset/QuoraDuplicate_comapratives-2.json', 'r') as fin:\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if (count < 200):\n",
    "            try:\n",
    "                l = \"\\\"or\\\"\"\n",
    "                if (line[4] == \"\\\"or\\\"\"):\n",
    "                    list_qw.append(line[1])\n",
    "            except:\n",
    "                print (\"lin\", line)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_qwestions = ['Which is better: Windows 7 or Windows 8?', \"Do you prefer charcoal or gas grills when cooking food?\", \"Which is better for a homeowner, an electric or a gas lawn mower?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers = []\n",
    "list_of_answers.append(\"The answer depends on you. From a technical standpoint, Window 7 is less stable and uses system resources less efficiently than Windows 8. Additionally Windows 8 can run Metro apps that cannot be used on Windows 7 machines. Basically, Windows 8 can do everything that Windows 7 can do and more as--mentioned by Sean Phạm.You do however have to learn some key new interface changes--for example hot corners and the start screen. These have a reputation for not being intuitive for mouse and keyboard users. I tend to disagree, because I use Windows 8 on a daily basis on my non-touchscreen desktop and laptop, for coding, gaming, web browsing, document authoring and other common tasks. I don't feel any slower with Windows 8 than Windows 7.If you're willing to learn the interface changes, then Windows 8 is a win-win situation. Otherwise you might want to stick with Windows 7 and forgo the stability improvements and other small improvements of Windows 8.\")\n",
    "list_of_answers.append(\"Gas bbq is clean, quick, and gets you out of the kitchen onto the deck in a more sociable way. It is however a little soulless. It’s mostly about the destination. Charcoal cooking is a far more social event, where a group of folk can stand around the flames and enjoy the whole journey, savouring the destination, the anticipation, the aroma, the flames, the feeling of comradery, and hearing the meat cooking, and smelling the meat and other foods under your tender care. It has soul, but it is slow, not as tidy, and far far more fun.\")\n",
    "list_of_answers.append(\"Gas, hands down. While I appreciate staying green, electric mowers just don’t have the power to cut through tough grass or weeds, to mulch clippings, or to mow high or thick grass if you missed a week. Corded versions are better than battery powered (there’s nothing worse than running out of juice halfway through the mowing), but then you’ll have to have a long extension cord to reach all areas of the lawn, and you’ll also have to be conscientious of where the cord is so you don’t run over it. Gas mowers have power, and if the tank runs out, you can refill it and keep ongoing immediately while you’re still waiting on a battery mower to charge. There’s no cord to worry about either.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers_template = []\n",
    "list_of_answers_template.append(\"i came to the conclusion that windows 8 is better, because: first, quicker, second, easier, third, smoother, fourth, faster.. But i should tell you that 7 is: first, greater, second, lighterfaster, third, worse for microsoft.\")\n",
    "list_of_answers_template.append(\"after much thought, I realized that  charcoal is better, because: lighter, greater, cheaper, taste.. But you should know that gas is: cleaner, easier to use, faster, quicker.\")\n",
    "list_of_answers_template.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is better: Windows 7 or Windows 8?\n",
      "9\n",
      "9\n",
      "in extractor get params 0\n",
      "LayerContextWordEmbeddingsBert 2\n",
      "LayerContextWordEmbeddingsBert2 2\n",
      "LayerContextWordEmbeddingsBert3 2\n",
      "LayerContextWordEmbeddingsBert4 2\n",
      "LayerContextWordEmbeddingsBert5 2\n",
      "LayerContextWordEmbeddingsBert6 2\n",
      "LayerContextWordEmbeddingsBert7 2\n",
      "LayerContextWordEmbeddingsBert8 2\n",
      "LayerContextWordEmbeddingsBert9 2\n",
      "extract_objects_predicates tags [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[]\n",
      "[]\n",
      "We try to use spacy\n",
      "split_sent ['which', 'is', 'better:', 'windows', '7', 'or', 'windows', '8?']\n",
      "tokens  ['Which', 'is', 'better', ':', 'Windows', '7', 'or', 'Windows', '8', '?']\n",
      "or index doc snet 6\n",
      "begin end  5 6 7\n",
      "obj1 spacy doc sent 7\n",
      "or index doc snet 6\n",
      "begin end  7 9 Windows 8\n",
      "obj2 spacy doc sent Windows 8\n",
      "9\n",
      "len(obj1), len(obj2) 1 9\n",
      "obj1, obj2, predicates 7 Windows 8 []\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': '7', 'objectB': 'Windows 8', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create fro json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['quicker', 'easier', 'smoother', 'faster'] ['greater', 'lighterfaster', 'worse for microsoft']\n",
      "2\n",
      "winnder: windows 8  other: 7\n",
      "acpect winner  quicker, easier, smoother, faster\n",
      "acpect other  greater, lighterfaster, worse for microsoft\n",
      "type  small\n",
      "winnder: windows 8  other: 7\n",
      "acpect winner  quicker, easier, smoother, faster\n",
      "acpect other  greater, lighterfaster, worse for microsoft\n",
      "self predicate  better\n",
      "answer begin:  The windows 8 is better than 7. The reason is quicker, easier, smoother, faster. \n",
      "self type small\n",
      "answer_begin The windows 8 is better than 7. The reason is quicker, easier, smoother, faster. \n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "exist3\n",
      "exist4\n",
      "exist\n",
      "Do you prefer charcoal or gas grills when cooking food?\n",
      "9\n",
      "9\n",
      "in extractor get params 0\n",
      "LayerContextWordEmbeddingsBert 2\n",
      "LayerContextWordEmbeddingsBert2 2\n",
      "LayerContextWordEmbeddingsBert3 2\n",
      "LayerContextWordEmbeddingsBert4 2\n",
      "LayerContextWordEmbeddingsBert5 2\n",
      "LayerContextWordEmbeddingsBert6 2\n",
      "LayerContextWordEmbeddingsBert7 2\n",
      "LayerContextWordEmbeddingsBert8 2\n",
      "LayerContextWordEmbeddingsBert9 2\n",
      "extract_objects_predicates tags [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[]\n",
      "[]\n",
      "We try to use spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_sent ['do', 'you', 'prefer', 'charcoal', 'or', 'gas', 'grills', 'when', 'cooking', 'food?']\n",
      "tokens  ['Do', 'you', 'prefer', 'charcoal', 'or', 'gas', 'grills', 'when', 'cooking', 'food', '?']\n",
      "or simple split_sent 4\n",
      "charcoal gas\n",
      "9\n",
      "len(obj1), len(obj2) 8 3\n",
      "obj1, obj2, predicates charcoal gas []\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'charcoal', 'objectB': 'gas', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create fro json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['greater', 'lighter', 'cheaper', 'taste'] ['easier to use', 'cleaner', 'faster', 'quicker']\n",
      "2\n",
      "winnder: charcoal  other: gas\n",
      "acpect winner  greater, lighter, cheaper, taste\n",
      "acpect other  easier to use, cleaner, faster, quicker\n",
      "type  small\n",
      "winnder: charcoal  other: gas\n",
      "acpect winner  greater, lighter, cheaper, taste\n",
      "acpect other  easier to use, cleaner, faster, quicker\n",
      "self predicate  better\n",
      "answer begin:  The charcoal is better than gas. The reason is greater, lighter, cheaper, taste. \n",
      "self type small\n",
      "answer_begin The charcoal is better than gas. The reason is greater, lighter, cheaper, taste. \n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "exist3\n",
      "exist4\n",
      "exist\n",
      "Which is better for a homeowner, an electric or a gas lawn mower?\n",
      "9\n",
      "9\n",
      "in extractor get params 0\n",
      "LayerContextWordEmbeddingsBert 2\n",
      "LayerContextWordEmbeddingsBert2 2\n",
      "LayerContextWordEmbeddingsBert3 2\n",
      "LayerContextWordEmbeddingsBert4 2\n",
      "LayerContextWordEmbeddingsBert5 2\n",
      "LayerContextWordEmbeddingsBert6 2\n",
      "LayerContextWordEmbeddingsBert7 2\n",
      "LayerContextWordEmbeddingsBert8 2\n",
      "LayerContextWordEmbeddingsBert9 2\n",
      "extract_objects_predicates tags [['O', 'O', 'B-PREDFULL', 'I-PREDFULL', 'I-PREDFULL', 'I-PREDFULL', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[]\n",
      "['better']\n",
      "We try to use spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_sent ['which', 'is', 'better', 'for', 'a', 'homeowner,', 'an', 'electric', 'or', 'a', 'gas', 'lawn', 'mower?']\n",
      "tokens  ['Which', 'is', 'better', 'for', 'a', 'homeowner', ',', 'an', 'electric', 'or', 'a', 'gas', 'lawn', 'mower', '?']\n",
      "or simple split_sent 9\n",
      "electric a\n",
      "9\n",
      "len(obj1), len(obj2) 8 1\n",
      "obj1, obj2, predicates electric a ['better']\n",
      "aspects ['better']\n",
      "weights [1]\n",
      "get url\n",
      "params {'objectA': 'electric', 'objectB': 'a', 'fs': 'true', 'aspect1': 'better', 'weight1': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create fro json predicates ['better']\n",
      "create fro json self.predicates better\n",
      "aspects  ['safer to use', 'easier to care for and causing much less air pollution', 'larger', 'lighter'] ['simpler', 'cheaper', 'smarter', 'healthier']\n",
      "2\n",
      "winnder: electric  other: a\n",
      "acpect winner  safer to use, easier to care for and causing much less air pollution, larger, lighter\n",
      "acpect other  simpler, cheaper, smarter, healthier\n",
      "type  small\n",
      "winnder: electric  other: a\n",
      "acpect winner  safer to use, easier to care for and causing much less air pollution, larger, lighter\n",
      "acpect other  simpler, cheaper, smarter, healthier\n",
      "self predicate  better\n",
      "answer begin:  The electric is better than a. The reason is safer to use, easier to care for and causing much less air pollution, larger, lighter. \n",
      "self type small\n",
      "answer_begin The electric is better than a. The reason is safer to use, easier to care for and causing much less air pollution, larger, lighter. \n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "exist3\n",
      "exist4\n",
      "exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_answers_small = []\n",
    "\n",
    "my_diviner = GPT2Small\n",
    "\n",
    "for elem in list_of_qwestions:\n",
    "    print (elem)\n",
    "    answer = generate_answer(my_extractor, my_diviner, elem)\n",
    "    list_of_answers_small.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smth wrong in answer generation, please try again',\n",
       " 'smth wrong in answer generation, please try again',\n",
       " 'smth wrong in answer generation, please try again']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_answers_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'rouge/requirements.txt'\n",
      "Hint: It looks like a path. File 'rouge/requirements.txt' does not exist.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rouge/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from rouge-score) (0.8.1)\n",
      "Requirement already satisfied: nltk in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from rouge-score) (3.4.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from rouge-score) (1.13.0)\n",
      "Requirement already satisfied: numpy in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from rouge-score) (1.17.4)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.0.3\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The quick brown dog jumps over the lazy fox.')\n",
    "print (scores)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The fast hazel hound skips above the inactive tod.')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'Just the place for a Snark the Bellman cried')\n",
    "print (scores)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The fast hazel hound skips above the inactive tod',\n",
    "                      'Just the place for a Snark the Bellman cried')\n",
    "print (scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.4, recall=0.08235294117647059, fmeasure=0.13658536585365855),\n",
       " 'rougeL': Score(precision=0.22857142857142856, recall=0.047058823529411764, fmeasure=0.07804878048780488)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(list_of_answers[0], list_of_answers_template[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook/cqas\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('/notebook/uncased_L-12_H-768_A-12/bert_model.ckpt.meta')\n",
    "    saver.restore(sess, \"/notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "usage: /opt/.pyenv/versions/3.7.4/bin/bert-serving-start -model_dir /notebook/uncased_L-12_H-768_A-12/ -num_worker=1\n",
      "                 ARG   VALUE\n",
      "__________________________________________________\n",
      "           ckpt_name = bert_model.ckpt\n",
      "         config_name = bert_config.json\n",
      "                cors = *\n",
      "                 cpu = False\n",
      "          device_map = []\n",
      "       do_lower_case = True\n",
      "  fixed_embed_length = False\n",
      "                fp16 = False\n",
      " gpu_memory_fraction = 0.5\n",
      "       graph_tmp_dir = None\n",
      "    http_max_connect = 10\n",
      "           http_port = None\n",
      "        mask_cls_sep = False\n",
      "      max_batch_size = 256\n",
      "         max_seq_len = 25\n",
      "           model_dir = /notebook/uncased_L-12_H-768_A-12/\n",
      "no_position_embeddings = False\n",
      "    no_special_token = False\n",
      "          num_worker = 1\n",
      "       pooling_layer = [-2]\n",
      "    pooling_strategy = REDUCE_MEAN\n",
      "                port = 5555\n",
      "            port_out = 5556\n",
      "       prefetch_size = 10\n",
      " priority_batch_size = 16\n",
      "show_tokens_to_client = False\n",
      "     tuned_model_dir = None\n",
      "             verbose = False\n",
      "                 xla = False\n",
      "\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: /notebook/uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: /notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /tmp/tmp7y9dd574\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /tmp/tmp7y9dd574\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> gpu  2\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device gpu: 2, load graph from /tmp/tmp7y9dd574\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n"
     ]
    }
   ],
   "source": [
    "! bert-serving-start -model_dir \"/notebook/uncased_L-12_H-768_A-12/\" -num_worker=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-serving-client in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from bert-serving-client) (1.18.1)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from bert-serving-client) (18.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BertClient() as bc:\n",
    "    # encode untokenized sentences\n",
    "    bc.encode(['First do it',\n",
    "              'then do it right',\n",
    "              'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()\n",
    "a = bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvcc: not found\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert embedding лучше учитывает контекст (так, что в словах \"Я лублю яблоки\" \"Мне нравятся яблочные макбуки\" Я - разное\n",
    "В случае с service bert I -одинаковое apple одинаковок\n",
    "\n",
    "I like apple\n",
    "I like apple macbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "bert_abstract = \"\"\"The quick brown dog jumps over the lazy fox.\\n The fast hazel hound skips above inactive tod.\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrgam ('jumps',)\n",
    "list_of_ngrams Counter({('the',): 2, ('just',): 1, ('place',): 1, ('for',): 1, ('a',): 1, ('snark',): 1, ('bellman',): 1, ('cried',): 1})\n",
    "[0.6792433]\n",
    "[0.8161626]\n",
    "[0.6827992]\n",
    "[0]\n",
    "[0.8117111]\n",
    "[0]\n",
    "[0]\n",
    "[0.62362134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding(['jumps', 'just', 'a'])\n",
    "vectors = result[0][1]\n",
    "vectors1 = result[1][1]\n",
    "vectors2 = result[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jumps']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67924374]], dtype=float32)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib1 = cosine_similarity(result[0][1][0].reshape(1, -1),result[1][1][0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81171143]], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(result[0][1][0].reshape(1, -1),result[2][1][0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7090132]], dtype=float32)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors1[0].reshape(1, -1),vectors2[0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hound'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = result[0][1]\n",
    "vectors1 = result[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7361618]], dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][0], result[1][0][0])\n",
    "cos_lib = cosine_similarity(vectors[0].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick fast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7143352]], dtype=float32)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][1])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[1].reshape(1, -1)) #similarity between #cat and dog\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown hazel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.62875855]], dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][2], result[1][0][2])\n",
    "cos_lib = cosine_similarity(vectors[2].reshape(1, -1),vectors1[2].reshape(1, -1)) #similarity between #mother and father\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick above\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.33970988]], dtype=float32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][5])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[5].reshape(1, -1)) #similarity between #mother and woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick inactive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.42793167]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][6])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[6].reshape(1, -1)) #similarity between #mother and woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6268022]], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vectors[8].reshape(1, -1),vectors1[3].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04705547, -0.3263731 ,  0.5511638 , -0.2329619 ,  0.4126335 ,\n",
       "        0.4367628 , -0.23546723, -0.10500348, -0.69917077, -0.20350926],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1][2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48047015,  0.56337065,  0.9621709 , -0.2585507 ,  1.7479268 ,\n",
       "       -0.00799218, -0.21425778,  0.7045478 , -0.08732209, -0.6905228 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][1][3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'work', 'on', 'apple', 'macbook']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_lib = cosine_similarity(vectors[1,:],vectors[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = BertClient()\n",
    "vectors = client.encode(['I like apple.'])\n",
    "vectors1 = client.encode(['I like apple macbook.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = client.encode(['the', 'quick', 'brown', 'dog', 'jumps', 'over', 'the', 'lazy', 'fox', '.'])\n",
    "vectors1 = client.encode(['the','fast', 'hazel', 'hound', 'skips', 'above', 'inactive', 'tod', '.'])\n",
    "vectors2 = client.encode(['the','woman', 'should', 'be', 'my', 'mother', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8320957]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_lib = cosine_similarity(vectors[7].reshape(1, -1),vectors2[1].reshape(1, -1)) #similarity between #mother and #woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3891856]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[2].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #father and man\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[0].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #the and the\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92401916]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[1].reshape(1, -1)) #similarity between #quick and fast\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8373927]], dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors1[2].reshape(1, -1),vectors[2].reshape(1, -1)) #similarity between #brown and hazel\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "если слова в n-граме не совпадают, то "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import porter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "stemmer = porter.PorterStemmer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def create_ngrams(tokens, n): #сюда добавть эмбединги\n",
    "    ngrams = collections.Counter()\n",
    "    ngrams_embs = collections.Counter()\n",
    "    for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "    \n",
    "\n",
    "def score(target, prediction, tp = 'Rouge1'):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'six' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f1f3b36d569c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'six' is not defined"
     ]
    }
   ],
   "source": [
    "for ngram in six.iterkeys(ngrams1):\n",
    "    print (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick\n",
      "quick brown\n",
      "brown dog\n",
      "dog jumps\n",
      "jumps over\n",
      "over the\n",
      "the lazy\n",
      "lazy fox\n"
     ]
    }
   ],
   "source": [
    "for elem in list(ngrams1.keys()):\n",
    "    print (' '.join(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = bert_embedding([' '.join(elem)for elem in list(ngrams1.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy just\n",
    "lazy jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown dog jumps over the lazy fox', 'The fast hazel hound skips above the inactive tod.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "bert_embedding = BertEmbedding()\n",
    "import collections\n",
    "\n",
    "str_1 = 'The quick brown dog jumps over the lazy fox'\n",
    "str_2 = 'The fast hazel hound skips above the inactive tod.'\n",
    "str_3 = 'Just the place for Snark the Bellman cried'\n",
    "print ([str_1, str_2])\n",
    "result = bert_embedding([str_1, str_2, str_3])\n",
    "ngrams1_embeddings = result[0][1]\n",
    "ngrams2_embeddings = result[1][1]\n",
    "ngrams3_embeddings = result[2][1]\n",
    "\n",
    "ngrams1 = create_ngrams(result[0][0], 2)\n",
    "ngrams2= create_ngrams(result[1][0], 2)\n",
    "ngrams3= create_ngrams(result[2][0], 2)\n",
    "\n",
    "ngrams01 = create_ngrams(result[0][0], 1)\n",
    "ngrams02= create_ngrams(result[1][0], 1)\n",
    "ngrams03= create_ngrams(result[2][0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'quick'): 1,\n",
       "         ('quick', 'brown'): 1,\n",
       "         ('brown', 'dog'): 1,\n",
       "         ('dog', 'jumps'): 1,\n",
       "         ('jumps', 'over'): 1,\n",
       "         ('over', 'the'): 1,\n",
       "         ('the', 'lazy'): 1,\n",
       "         ('lazy', 'fox'): 1})"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_target = bert_embedding([' '.join(elem)for elem in list(ngrams1.keys())])\n",
    "embeddings_predictions = bert_embedding([' '.join(elem)for elem in list(ngrams2.keys())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_target[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def fmeasure(precision, recall):\n",
    "  \"\"\"Computes f-measure given precision and recall values.\"\"\"\n",
    "\n",
    "  if precision + recall > 0:\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "  else:\n",
    "    return 0.0\n",
    "\n",
    "def cos_sim(emb1, emb2):\n",
    "    return cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))\n",
    "\n",
    "def count_ngram_overlap(ngram1_embs, ngram2_embs): #The idea count cousine similarity to every pair. If > 0.6 than add\n",
    "    result = 1\n",
    "    #print (\"ngram1_embs\", len(ngram1_embs), len(ngram1_embs[0]))\n",
    "    for elem in ngram1_embs: # по словам в н-граме\n",
    "        similarities = [cos_sim(elem, elem2)[0][0] if cos_sim(elem, elem2) >= 0.6 else 0 for elem2 in ngram2_embs]\n",
    "        #print (similarities)\n",
    "        max_ = max(similarities)\n",
    "        result *= max_\n",
    "    return result\n",
    "        \n",
    "\n",
    "def count_overlap(ngram, ngram_emb, list_of_ngrams2, list_of_ngrams2_embs): # only if ngram not in list_of_ngrams !!!\n",
    "    overlaps = [count_ngram_overlap(ngram_emb, elem[1]) for elem in list_of_ngrams2_embs] # по всем н-грамам\n",
    "    return max(overlaps)\n",
    "\n",
    "def score_ngrams(target_ngrams, prediction_ngrams):\n",
    "    intersection_ngrams_count = 0\n",
    "    \n",
    "    #print (\"target_ngrams\", target_ngrams)\n",
    "    #print (\"prediction_ngrams\", prediction_ngrams)\n",
    "    \n",
    "    embeddings_target = bert_embedding([' '.join(elem)for elem in list(target_ngrams.keys())])\n",
    "    embeddings_predictions = bert_embedding([' '.join(elem)for elem in list(prediction_ngrams.keys())])\n",
    "    \n",
    "    for ind, ngram in enumerate(list(target_ngrams.keys())):\n",
    "        #print (\"ind\", ind)\n",
    "        intersection_ngrams_count += min(target_ngrams[ngram],\n",
    "                                         prediction_ngrams[ngram])\n",
    "        if (min(target_ngrams[ngram], prediction_ngrams[ngram]) == 0):\n",
    "            #print (\"ngram\", ngram)\n",
    "            #print ('overlap')\n",
    "            overlap_ngram = count_overlap(ngram, embeddings_target[ind][1], prediction_ngrams, embeddings_predictions) #по всем н-грамам\n",
    "            #print ('overlap ngram', overlap_ngram)\n",
    "            intersection_ngrams_count += overlap_ngram\n",
    "            \n",
    "    target_ngrams_count = sum(target_ngrams.values())\n",
    "    prediction_ngrams_count = sum(prediction_ngrams.values())\n",
    "    \n",
    "    #print (\"intersection_ngrams_count\", intersection_ngrams_count)\n",
    "    #print (\"intersection_targets_count\", prediction_ngrams_count, target_ngrams_count)\n",
    "\n",
    "\n",
    "    precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
    "    recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
    "    print (precision, recall)\n",
    "    \n",
    "    f = fmeasure(precision, recall)\n",
    "    return f, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794479370117188 0.7549421522352431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7152083547491778, 0.6794479370117188, 0.7549421522352431)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794479370117188 0.7549421522352431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7152083547491778, 0.6794479370117188, 0.7549421522352431)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5479773506522179 0.487090978357527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5157433888491462, 0.5479773506522179, 0.487090978357527)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.125, recall=0.1111111111111111, fmeasure=0.11764705882352941), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The fast hazel hound skips above inactive tod.')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown dog jumps over the lazy fox.', ' The fast hazel hound skips above inactive tod. ', ' Just the place for a Snark the Bellman cried']\n"
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "bert_abstract = \"\"\"The quick brown dog jumps over the lazy fox.\\n The fast hazel hound skips above inactive tod. \\n Just the place for a Snark the Bellman cried\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "print (sentences)\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from six.moves import map\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'quick')\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'intersection_ngrams_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-0781affcb4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mintersection_ngrams_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intersection_ngrams_count' is not defined"
     ]
    }
   ],
   "source": [
    "for ngram in six.iterkeys(ngrams1):\n",
    "    print (ngram)\n",
    "    print (ngrams1[ngram])\n",
    "    print (ngrams2[ngram])\n",
    "    print (min(ngrams1[ngram], ngrams2[ngram]))\n",
    "    intersection_ngrams_count += min(ngrams1[ngram], ngrams2[ngram])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
    "      \"\"\"Compute n-gram based rouge scores.\n",
    "      Args:\n",
    "        target_ngrams: A Counter object mapping each ngram to number of\n",
    "          occurrences for the target text.\n",
    "        prediction_ngrams: A Counter object mapping each ngram to number of\n",
    "          occurrences for the prediction text.\n",
    "      Returns:\n",
    "        A Score object containing computed scores.\n",
    "      \"\"\"\n",
    "\n",
    "      intersection_ngrams_count = 0\n",
    "      for ngram in six.iterkeys(target_ngrams):\n",
    "        intersection_ngrams_count += min(target_ngrams[ngram],\n",
    "                                         prediction_ngrams[ngram])\n",
    "      target_ngrams_count = sum(target_ngrams.values())\n",
    "      prediction_ngrams_count = sum(prediction_ngrams.values())\n",
    "\n",
    "      precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
    "      recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
    "      fmeasure = scoring.fmeasure(precision, recall)\n",
    "\n",
    "      return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>object_a</th>\n",
       "      <th>object_b</th>\n",
       "      <th>Q1 url</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2 url</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3 url</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4 url</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5 url</th>\n",
       "      <th>Q5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brands</td>\n",
       "      <td>toyota</td>\n",
       "      <td>honda</td>\n",
       "      <td>https://www.quora.com/Which-is-a-more-reliable...</td>\n",
       "      <td>I wrote this to answer another question on Quo...</td>\n",
       "      <td>https://www.quora.com/Which-is-a-more-reliable...</td>\n",
       "      <td>I drive a Honda and my wife a Toyota. So by ow...</td>\n",
       "      <td>https://www.autogravity.com/autogravitas/cars/...</td>\n",
       "      <td>In terms of sheer corporate value, Toyota is t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brands</td>\n",
       "      <td>nokia</td>\n",
       "      <td>motorola</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Motorola has a history of making good android ...</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Well, it depends what you expect from a phone....</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Motorola is best .Its always a branded better ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brands</td>\n",
       "      <td>nokia</td>\n",
       "      <td>samsung</td>\n",
       "      <td>https://www.theverge.com/2019/9/2/20844444/and...</td>\n",
       "      <td>Share All sharing options for: Nokia is better...</td>\n",
       "      <td>https://www.quora.com/Is-Nokia-better-than-Sam...</td>\n",
       "      <td>At present Samsung is better than Nokia.Since,...</td>\n",
       "      <td>https://www.quora.com/Is-Nokia-better-than-Sam...</td>\n",
       "      <td>In my opinion i would say Nokia.. If you want ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compsci</td>\n",
       "      <td>Javascript</td>\n",
       "      <td>PHP</td>\n",
       "      <td>https://www.bitdegree.org/tutorials/php-vs-jav...</td>\n",
       "      <td>In the PHP vs. JavaScript discussion, it is im...</td>\n",
       "      <td>https://www.quora.com/Is-PHP-better-than-JavaS...</td>\n",
       "      <td>I would normally say “it depends” for a questi...</td>\n",
       "      <td>https://www.quora.com/Is-PHP-better-than-JavaS...</td>\n",
       "      <td>PHP is generally easier for people to learn si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>compsci</td>\n",
       "      <td>Perl</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>https://www.educba.com/perl-vs-ruby/</td>\n",
       "      <td>Perl is an ideal choice for system administrat...</td>\n",
       "      <td>https://www.quora.com/As-someone-who-knows-Rub...</td>\n",
       "      <td>I know a lot more Ruby than Perl, but the prim...</td>\n",
       "      <td>https://www.coursereport.com/blog/ruby-vs-pyth...</td>\n",
       "      <td>https://www.quora.com/As-someone-who-knows-Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain    object_a  object_b  \\\n",
       "0   brands      toyota     honda   \n",
       "1   brands       nokia  motorola   \n",
       "2   brands       nokia   samsung   \n",
       "3  compsci  Javascript       PHP   \n",
       "4  compsci        Perl      Ruby   \n",
       "\n",
       "                                              Q1 url  \\\n",
       "0  https://www.quora.com/Which-is-a-more-reliable...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.theverge.com/2019/9/2/20844444/and...   \n",
       "3  https://www.bitdegree.org/tutorials/php-vs-jav...   \n",
       "4               https://www.educba.com/perl-vs-ruby/   \n",
       "\n",
       "                                                  Q1  \\\n",
       "0  I wrote this to answer another question on Quo...   \n",
       "1  Motorola has a history of making good android ...   \n",
       "2  Share All sharing options for: Nokia is better...   \n",
       "3  In the PHP vs. JavaScript discussion, it is im...   \n",
       "4  Perl is an ideal choice for system administrat...   \n",
       "\n",
       "                                              Q2 url  \\\n",
       "0  https://www.quora.com/Which-is-a-more-reliable...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.quora.com/Is-Nokia-better-than-Sam...   \n",
       "3  https://www.quora.com/Is-PHP-better-than-JavaS...   \n",
       "4  https://www.quora.com/As-someone-who-knows-Rub...   \n",
       "\n",
       "                                                  Q2  \\\n",
       "0  I drive a Honda and my wife a Toyota. So by ow...   \n",
       "1  Well, it depends what you expect from a phone....   \n",
       "2  At present Samsung is better than Nokia.Since,...   \n",
       "3  I would normally say “it depends” for a questi...   \n",
       "4  I know a lot more Ruby than Perl, but the prim...   \n",
       "\n",
       "                                              Q3 url  \\\n",
       "0  https://www.autogravity.com/autogravitas/cars/...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.quora.com/Is-Nokia-better-than-Sam...   \n",
       "3  https://www.quora.com/Is-PHP-better-than-JavaS...   \n",
       "4  https://www.coursereport.com/blog/ruby-vs-pyth...   \n",
       "\n",
       "                                                  Q3  Q4 url  Q4  Q5 url  Q5  \n",
       "0  In terms of sheer corporate value, Toyota is t...     NaN NaN     NaN NaN  \n",
       "1  Motorola is best .Its always a branded better ...     NaN NaN     NaN NaN  \n",
       "2  In my opinion i would say Nokia.. If you want ...     NaN NaN     NaN NaN  \n",
       "3  PHP is generally easier for people to learn si...     NaN NaN     NaN NaN  \n",
       "4  https://www.quora.com/As-someone-who-knows-Rub...     NaN NaN     NaN NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Read data from file 'filename.csv' \n",
    "# (in the same directory that your python process is based)\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(\"Evaluation dataset - Sheet1.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = data['Q2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wrote this to answer another question on Quora but it very effectively addresses this question as well. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2 = data['Q3'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_templates = 'After much thought, I realized that Honda is better, because: quicker, bigger, nicer, weaker, but toyota is lighter, easier, faster, softer.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = \"I've used the Honda the past few years, and they're pretty good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and they're better.have been using the Honda for a while, and I can tell you that they're very good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and thy're better. They're just not as good as the Honda.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = \"They don\\'t have that kind of engine. Maybe they are trying to make this bigger, faster, more powerful. You cannot afford to have a car with a big engine. You need to be careful when you make a big engine. There is no point, because it will explode. There is no point in spending a lot of money on a big engine.(CNN) After the shooting deaths of five police officers in Dallas last week, President Barack Obama offered condolences to the families of the fallen officers, calling the situation an attack on our shared humanity.Yet more than two months after the deaths of Alton Sterling in Louisiana and Philando Castile in Minnesota, the President did not issue a statement or call for unity or reflection. While we mourn for the officers who lost their lives in Dallas, we do not yet know the full extent of the threat that this attack represents, Obama said in a statement.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've used the Honda the past few years, and they're pretty good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and they're better.have been using the Honda for a while, and I can tell you that they're very good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and thy're better. They're just not as good as the Honda.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding([gen_templates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import porter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "stemmer = porter.PorterStemmer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ngrams1 = create_ngrams(tokenizer.tokenize(gen_templates), 2)\n",
    "ngrams2= create_ngrams(tokenizer.tokenize(small), 2)\n",
    "ngrams3= create_ngrams(tokenizer.tokenize(big), 2)\n",
    "\n",
    "ngrams01 = create_ngrams(tokenizer.tokenize(gen_templates), 1)\n",
    "ngrams02= create_ngrams(tokenizer.tokenize(small), 1)\n",
    "ngrams03= create_ngrams(tokenizer.tokenize(big), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After much thought, I realized that Honda is better, because: quicker, bigger, nicer, weaker, but Toyota is lighter, easier, faster, softer.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngrams1_t = create_ngrams(tokenizer.tokenize(target1), 2)\n",
    "ngrams2_t= create_ngrams(tokenizer.tokenize(target2), 2)\n",
    "\n",
    "ngrams01_t = create_ngrams(tokenizer.tokenize(target1), 1)\n",
    "ngrams02_t = create_ngrams(tokenizer.tokenize(target2), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0821059115319384 0.6491498630493879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.14577400433389767, 0.0821059115319384, 0.6491498630493879)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38255592389982573 0.5857887584716082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4628454387923817, 0.38255592389982573, 0.5857887584716082)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22225900106279275 0.5020672077579158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30811795763773464, 0.22225900106279275, 0.5020672077579158)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams02, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212211319378444 0.31553424522280693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.439004167266514, 0.7212211319378444, 0.31553424522280693)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams02, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4667321349321147 0.6709274439649149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5505045694071096, 0.4667321349321147, 0.6709274439649149)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams03, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6459892696263838 0.4582583762028001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7169197707706028, 1.6459892696263838, 0.4582583762028001)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams03, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('I', 'drive'): 1,\n",
       "         ('drive', 'a'): 1,\n",
       "         ('a', 'Honda'): 1,\n",
       "         ('Honda', 'and'): 1,\n",
       "         ('and', 'my'): 1,\n",
       "         ('my', 'wife'): 1,\n",
       "         ('wife', 'a'): 1,\n",
       "         ('a', 'Toyota'): 1,\n",
       "         ('Toyota', '.'): 1,\n",
       "         ('.', 'So'): 1,\n",
       "         ('So', 'by'): 1,\n",
       "         ('by', 'owning'): 1,\n",
       "         ('owning', 'both'): 1,\n",
       "         ('both', 'cars'): 1,\n",
       "         ('cars', 'I'): 1,\n",
       "         ('I', 'have'): 1,\n",
       "         ('have', 'experienced'): 1,\n",
       "         ('experienced', 'both'): 1,\n",
       "         ('both', 'brands'): 2,\n",
       "         ('brands', '.'): 2,\n",
       "         ('.', 'I'): 1,\n",
       "         ('I', '’'): 1,\n",
       "         ('’', 've'): 1,\n",
       "         ('ve', 'been'): 1,\n",
       "         ('been', 'driving'): 1,\n",
       "         ('driving', 'a'): 1,\n",
       "         ('a', 'honda'): 1,\n",
       "         ('honda', 'city'): 1,\n",
       "         ('city', 'AT'): 1,\n",
       "         ('AT', 'for'): 1,\n",
       "         ('for', 'almost'): 1,\n",
       "         ('almost', '7'): 1,\n",
       "         ('7', 'years'): 1,\n",
       "         ('years', 'and'): 1,\n",
       "         ('and', 'have'): 1,\n",
       "         ('have', 'never'): 1,\n",
       "         ('never', 'faced'): 1,\n",
       "         ('faced', 'any'): 1,\n",
       "         ('any', 'challenge'): 1,\n",
       "         ('challenge', 'whatsoever'): 1,\n",
       "         ('whatsoever', 'with'): 1,\n",
       "         ('with', 'the'): 1,\n",
       "         ('the', 'car'): 1,\n",
       "         ('car', '.'): 2,\n",
       "         ('.', 'The'): 3,\n",
       "         ('The', 'only'): 1,\n",
       "         ('only', 'major'): 1,\n",
       "         ('major', 'expenses'): 1,\n",
       "         ('expenses', 'include'): 1,\n",
       "         ('include', 'Two'): 1,\n",
       "         ('Two', 'sets'): 1,\n",
       "         ('sets', 'of'): 1,\n",
       "         ('of', 'tyres'): 1,\n",
       "         ('tyres', ','): 1,\n",
       "         (',', 'Insurance'): 1,\n",
       "         ('Insurance', 'And'): 1,\n",
       "         ('And', 'fuel'): 1,\n",
       "         ('fuel', '.'): 1,\n",
       "         ('The', 'car'): 1,\n",
       "         ('car', 'has'): 1,\n",
       "         ('has', 'done'): 2,\n",
       "         ('done', 'over'): 1,\n",
       "         ('over', '110k'): 1,\n",
       "         ('110k', 'and'): 1,\n",
       "         ('and', 'still'): 1,\n",
       "         ('still', 'drives'): 1,\n",
       "         ('drives', 'like'): 1,\n",
       "         ('like', 'a'): 2,\n",
       "         ('a', 'dream'): 1,\n",
       "         ('dream', '.'): 1,\n",
       "         ('.', 'Toyota'): 1,\n",
       "         ('Toyota', 'on'): 1,\n",
       "         ('on', 'the'): 2,\n",
       "         ('the', 'other'): 2,\n",
       "         ('other', 'hand'): 2,\n",
       "         ('hand', 'being'): 1,\n",
       "         ('being', 'twice'): 1,\n",
       "         ('twice', 'as'): 1,\n",
       "         ('as', 'expensive'): 1,\n",
       "         ('expensive', 'feels'): 1,\n",
       "         ('feels', 'like'): 2,\n",
       "         ('a', 'premium'): 1,\n",
       "         ('premium', 'car'): 1,\n",
       "         ('car', 'and'): 1,\n",
       "         ('and', 'is'): 1,\n",
       "         ('is', 'rock'): 1,\n",
       "         ('rock', 'solid'): 1,\n",
       "         ('solid', 'trouble'): 1,\n",
       "         ('trouble', 'free'): 1,\n",
       "         ('free', ','): 1,\n",
       "         (',', 'worry'): 1,\n",
       "         ('worry', 'free'): 1,\n",
       "         ('free', 'car'): 1,\n",
       "         ('The', 'biggest'): 1,\n",
       "         ('biggest', 'expense'): 1,\n",
       "         ('expense', 'here'): 1,\n",
       "         ('here', 'was'): 1,\n",
       "         ('was', 'insurance'): 1,\n",
       "         ('insurance', '.'): 1,\n",
       "         ('.', 'This'): 1,\n",
       "         ('This', 'one'): 1,\n",
       "         ('one', 'has'): 1,\n",
       "         ('done', '25k'): 1,\n",
       "         ('25k', 'and'): 1,\n",
       "         ('and', 'feels'): 1,\n",
       "         ('like', 'We'): 1,\n",
       "         ('We', 'bought'): 1,\n",
       "         ('bought', 'it'): 1,\n",
       "         ('it', 'yesterday'): 1,\n",
       "         ('yesterday', '.'): 1,\n",
       "         ('.', 'In'): 1,\n",
       "         ('In', 'terms'): 1,\n",
       "         ('terms', 'of'): 1,\n",
       "         ('of', 'comfort'): 1,\n",
       "         ('comfort', 'Toyota'): 1,\n",
       "         ('Toyota', 'is'): 1,\n",
       "         ('is', 'far'): 1,\n",
       "         ('far', 'superior'): 1,\n",
       "         ('superior', 'than'): 1,\n",
       "         ('than', 'any'): 1,\n",
       "         ('any', 'car'): 2,\n",
       "         ('car', 'in'): 2,\n",
       "         ('in', 'the'): 2,\n",
       "         ('the', 'industry'): 1,\n",
       "         ('industry', 'be'): 1,\n",
       "         ('be', 'it'): 1,\n",
       "         ('it', 'the'): 1,\n",
       "         ('the', 'driver'): 1,\n",
       "         ('driver', 'or'): 1,\n",
       "         ('or', 'the'): 1,\n",
       "         ('the', 'passengers'): 2,\n",
       "         ('passengers', 'there'): 1,\n",
       "         ('there', 'is'): 1,\n",
       "         ('is', 'no'): 2,\n",
       "         ('no', 'match'): 1,\n",
       "         ('match', 'for'): 1,\n",
       "         ('for', 'Innova'): 1,\n",
       "         ('Innova', '.'): 1,\n",
       "         ('.', 'Honda'): 1,\n",
       "         ('Honda', 'city'): 1,\n",
       "         ('city', 'on'): 1,\n",
       "         ('hand', 'is'): 1,\n",
       "         ('is', 'superb'): 1,\n",
       "         ('superb', 'in'): 1,\n",
       "         ('in', 'driving'): 1,\n",
       "         ('driving', '.'): 1,\n",
       "         ('.', 'It'): 1,\n",
       "         ('It', '’'): 1,\n",
       "         ('’', 's'): 1,\n",
       "         ('s', 'steering'): 1,\n",
       "         ('steering', 'is'): 1,\n",
       "         ('is', 'so'): 1,\n",
       "         ('so', 'light'): 1,\n",
       "         ('light', 'that'): 1,\n",
       "         ('that', 'anyone'): 1,\n",
       "         ('anyone', 'can'): 1,\n",
       "         ('can', 'drive'): 1,\n",
       "         ('drive', 'it'): 1,\n",
       "         ('it', 'with'): 1,\n",
       "         ('with', 'ease'): 1,\n",
       "         ('ease', '.'): 1,\n",
       "         ('.', 'There'): 1,\n",
       "         ('There', 'is'): 1,\n",
       "         ('no', 'engine'): 1,\n",
       "         ('engine', 'sound'): 1,\n",
       "         ('sound', 'at'): 1,\n",
       "         ('at', 'idling'): 1,\n",
       "         ('idling', '.'): 1,\n",
       "         ('.', 'Super'): 1,\n",
       "         ('Super', 'smooth'): 1,\n",
       "         ('smooth', 'ride'): 1,\n",
       "         ('ride', 'and'): 1,\n",
       "         ('and', 'you'): 1,\n",
       "         ('you', 'reach'): 1,\n",
       "         ('reach', 'in'): 1,\n",
       "         ('in', 'comfort'): 1,\n",
       "         ('comfort', 'to'): 1,\n",
       "         ('to', 'your'): 1,\n",
       "         ('your', 'destination'): 1,\n",
       "         ('destination', '.'): 1,\n",
       "         ('.', 'Long'): 1,\n",
       "         ('Long', 'drives'): 1,\n",
       "         ('drives', 'for'): 1,\n",
       "         ('for', 'the'): 1,\n",
       "         ('passengers', 'is'): 1,\n",
       "         ('is', 'not'): 1,\n",
       "         ('not', 'as'): 1,\n",
       "         ('as', 'good'): 1,\n",
       "         ('good', 'as'): 1,\n",
       "         ('as', 'Innova'): 1,\n",
       "         ('Innova', 'though'): 1,\n",
       "         ('though', '.'): 1,\n",
       "         ('.', 'Overall'): 1,\n",
       "         ('Overall', 'to'): 1,\n",
       "         ('to', 'sum'): 1,\n",
       "         ('sum', 'up'): 1,\n",
       "         ('up', 'both'): 1,\n",
       "         ('both', 'are'): 1,\n",
       "         ('are', 'amazing'): 1,\n",
       "         ('amazing', 'brands'): 1,\n",
       "         ('brands', 'super'): 1,\n",
       "         ('super', 'reliable'): 1,\n",
       "         ('reliable', ','): 1,\n",
       "         (',', 'great'): 1,\n",
       "         ('great', 'resale'): 1,\n",
       "         ('resale', 'value'): 1,\n",
       "         ('value', 'for'): 1,\n",
       "         ('for', 'both'): 1,\n",
       "         ('.', 'Service'): 1,\n",
       "         ('Service', 'cost'): 1,\n",
       "         ('cost', 'is'): 1,\n",
       "         ('is', 'comparable'): 1,\n",
       "         ('comparable', 'to'): 1,\n",
       "         ('to', 'any'): 1,\n",
       "         ('the', 'segment'): 1,\n",
       "         ('segment', '.'): 1,\n",
       "         ('.', 'To'): 1,\n",
       "         ('To', 'decide'): 1,\n",
       "         ('decide', 'between'): 1,\n",
       "         ('between', 'one'): 1,\n",
       "         ('one', 'of'): 1,\n",
       "         ('of', 'the'): 1,\n",
       "         ('the', 'two'): 1,\n",
       "         ('two', 'is'): 1,\n",
       "         ('is', 'a'): 1,\n",
       "         ('a', 'difficult'): 1,\n",
       "         ('difficult', 'decision'): 1,\n",
       "         ('decision', 'as'): 1,\n",
       "         ('as', 'both'): 1,\n",
       "         ('both', 'the'): 1,\n",
       "         ('the', 'brands'): 1,\n",
       "         ('brands', 'are'): 1,\n",
       "         ('are', 'of'): 1,\n",
       "         ('of', 'amazing'): 1,\n",
       "         ('amazing', 'quality'): 1,\n",
       "         ('quality', '.'): 1})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(gen_templates, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.025974025974025976, recall=0.2857142857142857, fmeasure=0.04761904761904762),\n",
       " 'rouge2': Score(precision=0.004347826086956522, recall=0.05, fmeasure=0.008)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.17316017316017315, recall=0.3669724770642202, fmeasure=0.2352941176470588),\n",
       " 'rouge2': Score(precision=0.02608695652173913, recall=0.05555555555555555, fmeasure=0.03550295857988165)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(small, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.23809523809523808, recall=0.34375, fmeasure=0.28132992327365725),\n",
       " 'rouge2': Score(precision=0.02608695652173913, recall=0.03773584905660377, fmeasure=0.030848329048843187)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(big, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.2857142857142857, recall=0.025974025974025976, fmeasure=0.04761904761904762),\n",
       " 'rouge2': Score(precision=0.05, recall=0.004347826086956522, fmeasure=0.008)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(target1, gen_templates)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In terms of sheer corporate value, Toyota is the most successful, preponderant automaker in the world. Honda is much smaller with an overall value that's just 25 percent of Toyota's. Toyota also sells many more vehicles in the United States every year than Honda.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
